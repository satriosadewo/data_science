## In this session, I would like to show how to check the OLS assumption on the problematic variables. This session is a continuation from the previous --> https://github.com/satriosadewo/data_science/blob/master/Preprocessing%20(determining%20the%20variable%20of%20interest%20and%20dealing%20with%20the%20outliers)
## Case Study : Price Determination on Used Car

## 1. Checking the linearity of the variables (price-year, price-engineV, price-mileage), by showing the scatter plot
f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))
ax1.scatter(data_cleaned['Year'],data_cleaned['Price'])
ax1.set_title('Price and Year')
ax2.scatter(data_cleaned['EngineV'],data_cleaned['Price'])
ax2.set_title('Price and EngineV')
ax3.scatter(data_cleaned['Mileage'],data_cleaned['Price'])
ax3.set_title('Price and Mileage')
plt.show()

## 2. We can see that the look of scatter plot doesnt look the linearity. So, we need to transform one or more variables. We check the price variable first, because this variable correlated with the other variables. 
sns.displot(data_cleaned['Price'])

## 3. The scatter plot of the price variable shows that the distribution is not normal, so we need to transform it using log transformation.
log_price = np.log(data_cleaned['Price'])
data_cleaned['log_price'] = log_price
data_cleaned

## 4. Normality assumption is OK because the population is very big. Zero mean assumption accomplished by the inclusion of the intercept in regression. The homoscedasticity also accomplised as we can see in the graph. No autocorrelation also accomplished because the data is not coming from time series or panel data.
## 5. Then, we check the multicollinearity through VIF (variance inflation factor). At this time, the variables that contain numerical data could be analyzed (Mileage, year, and engineV)
from statsmodels.stats.outliers_influence import variance_inflation_factor
variables = data_cleaned[['Mileage','Year','EngineV']]
vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif["features"] = variables.columns
vif

## 6. Intrepreting the VIF result, we can use this parameter. If the VIF = 1, so there is no multicollinearity. If the VIF between 1 and 5, so this is perfectly ok. If the VIF > 10, the variable is not unaccepted. The result of VIF shows that year variable have VIF 10.35, so we need to remove it.
data_no_multicollinearity = data_cleaned.drop(['Year'],axis=1)

## No Endogenety assumption will be discussed after the regression model is created.
